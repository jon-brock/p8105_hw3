---
title: 'Data Science - Homework #3'
author: "Jon Brock - JPB2210"
output: 
    github_document:
        toc: TRUE
        toc_depth: 2
---

> *"Die Grenzen meiner Sprache bedeuten die Grenzen meiner Welt." - Ludwig Wittgenstein, Tractatus Logico-Philosophicus*

```{r load_packages, message = FALSE}
library(tidyverse)
library(lubridate)
```

***

## Problem #1
###### (*25 points*)
##### *Exploration of the Instacart Data*

The following code imports the `instacart` dataset from the courses' `p8105.datasets` github site. As we all know, Instacart is the online service that allows consumers to order grocery items from a store of their choosing, have someone pull the items from the shelf, bag them, and either deliver them or set them aside for fast and easy pickup.

Given how massive the dataset is for this problem, we are going to pull it from the `p8105.datasets` library and assign it as the `instacart` tibble, as well as remove an unnecessary column `eval_set`.  

*Note: Despite the instructions to load the `instacart` dataset via `library(p8105.datasets)` I opted to use an equivalent method to which I am partial. I hope this does not result in any lost points.*

```{r generate_tibble_from_source_data_instacart}
instacart <- as_tibble(p8105.datasets::instacart) %>% 
    select(-eval_set)
```

***

The dataset `instacart` has `r ncol(instacart)` variables. There are variables that are specific to the items, such as `product_id`, `department`, `product_name`, and `aisle`. There are also variables that are specific to the consumer, such as `reordered`, `user_id`, `days_since_prior_order`, and `order_dow`.  This dataset has `r instacart %>% distinct(user_id) %>% count()` unique customers.

```{r high_order_aisles}
instacart %>% 
    group_by(aisle) %>% 
    summarise(aisle_count = n()) %>% 
    arrange(desc(aisle_count))
```

As we can see from our previous code chunk, there are `r instacart %>% distinct(aisle) %>% count()` aisles.
And the top five aisles from which items are ordered are:

1. fresh vegetables  
1. fresh fruits  
1. packaged vegetable fruits  
1. yogurt  
1. packaged cheese  

***

We can get a graphical representation of the top aisles ordered from using the following code and interpreting the generated graphic.  

```{r instacart_aisle_plot, fig.width = 9, fig.asp = 0.687, dpi = 200, fig.align = 'center'}
instacart %>%
    group_by(aisle) %>% 
    summarize(aisle_count = n()) %>% 
    filter(aisle_count > 10000) %>%
    ggplot() +
        geom_col(aes(x = aisle_count, y = reorder(aisle, -aisle_count))) + 
        labs(
            title = "Most Ordered Item Categories via Instacart",
            x = "Number of Items Ordered",
            y = "Aisle")
```

***

The following code chunk involves several steps, so I assigned it to its own tibble (`insta_top_product_tbl`). We sought to find the most ordered item from three specified aisles: `baking ingredients`, `dog food care`, and `packaged vegetables fruits`. As such, this particular code chunk merely prepares us for generating the required table.

```{r generate_top_product_tibble}
insta_top_product_tbl <-
    instacart %>% 
    select(aisle, product_name) %>% 
    filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
    group_by(aisle, product_name) %>% 
    mutate(product_count = n()) %>% 
    arrange(aisle, desc(product_count)) %>%
    distinct() %>% 
    group_by(aisle) %>% 
    filter(product_count == max(product_count))
```

Now that we've generated our tibble with our desired specifications, we can write a short code to tidy up and neatly present our findings.

```{r display_top_product_tibble_table}
insta_top_product_tbl %>% 
    mutate(product_name = str_to_lower(product_name)) %>% 
    rename(
        "Aisle" = aisle,
        "Most Ordered Item" = product_name,
        "Number of Times Ordered" = product_count) %>% 
    knitr::kable()
```

***

Next, we want to know what the mean hour time is for when customers purchase either `Pink Lady Apples` or `Coffee Ice Cream` -- yummy!

```{r mean_order_time_for_specified_products}
instacart %>% 
    select(product_name, order_dow, order_hour_of_day) %>% 
    filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
    mutate(
        order_dow = order_dow + 1,
        dow = wday(order_dow, label = TRUE, abbr = FALSE)) %>% 
    group_by(product_name, dow) %>% 
    arrange(product_name, dow) %>% 
    select(product_name, dow, everything()) %>% 
    summarize(
        mean_hour = as.integer(mean(order_hour_of_day))) %>% 
    pivot_wider(
        names_from = "dow",
        values_from = "mean_hour") %>% 
    rename("Product" = product_name) %>% 
    knitr::kable(align = 'c')
```

Our findings are presented in 24-hour time, which is a terribly underutilized method of time keeping. There can never be any ambiguity! None of this, "I set my alarm for 7:00 p.m. instead of 7:00 a.m., and that's why I'm late for work" nonsense. 19:00 is 19:00. Bam! Anyway, I digress. Our data indicate that around the middle of the week customers tend to order `Coffee Ice Cream` around 15:00, whereas `Pink Lady Apples` are typically ordered around 11:00 during the week.

***

## Problem #2
###### (*25 points*)
##### *Exploration of the Behavioral Risk Factors Surveillance System Data*

The following code imports the `brfss` dataset from the courses' `p8105.datasets` github site. We clean the column/variable names; filter the dataset down to only the `"Overall Health"` topic; and coerce and relevel the `response` (character) variable into a factor variable with reordered factors going from `"Poor"` to "`Excellent"`.

**Dataset of how many people rated their "general health" as** `Poor`, `Fair`, `Good`, `Very good`, **or** `Excellent` **within each county and state.**

*Note: Again, I loaded the dataset `brfss_smart2010` using the same process as I did in Problem #1.*

```{r generate_tibble_from_source_data_brfss}
#CODE CHUNK COMPLETED
brfss <- as_tibble(p8105.datasets::brfss_smart2010) %>% 
    janitor::clean_names() %>% 
    filter(topic == "Overall Health") %>% 
    mutate(
        response = fct_relevel(response, c("Poor", "Fair", "Good", "Very good", "Excellent")),
        county = str_to_lower(str_sub(locationdesc, 6, -1)),
        lat = str_sub(geo_location, 2, 10),
        lng = str_sub(geo_location, 13, 22)) %>% 
    rename(
        data_value_pct = data_value,
        state = locationabbr,
        cl_lower = confidence_limit_low,
        cl_upper = confidence_limit_high) %>% 
    select(year, state, county, response, sample_size, data_value_pct, respid, lat, lng)
```

Additionally, in terms of cleaning this dataset, there were far too many unnecessary variables. What started as a running list of specific variables to drop turned into a list of specific variables to keep. It is also good practice to have simple and descriptive variable (and sometimes observation) names (and values).  

As such, I removed the leading state abbreviations from `locationdesc` and renamed it accordingly. After running `count()` on several variables, such as `class` (all observations = "Health Status"), `topic` (all observations = "Overall Health"), and `question` (all observations = "How is your general health?"), I saw no point in keeping columns with no differing observations. Keep it tidy!  

(Bonus: though it wasn't asked for, I split the `geo_location` variable into its two requisite parts: `lat` and `lng`. Though, I do see that for some observations, the last digit of the longitude value is truncated. It maps the same location regardless.)

***

The following code answers the question: "In **2002**, which states were observed at 7 or more locations?"  

```{r observation_numbers_by_location_2002}
#CODE CHUNK COMPLETED
brfss %>% 
    filter(year == "2002") %>% 
    group_by(state) %>%
    summarize(locations = n_distinct(county)) %>% 
    filter(locations >= 7)
```

The answer: `CT` (Connecticut), `FL` (Florida), `MA` (Massachusetts), `NC` (North Carolina), `NJ` (New Jersey), and `PA` (Pennsylvania).  

***

The following code answers the question: "In **2010**, which states were observed at 7 or more locations?" 

```{r observation_numbers_by_location_2010}
#CODE CHUNK COMPLETED
brfss %>% 
    filter(year == "2010") %>% 
    group_by(state) %>%
    summarize(locations = n_distinct(county)) %>% 
    filter(locations >= 7)
```

The answer: `CA` (California), `CO` (Colorado), `FL` (Florida), `MA` (Massachusetts), `MD` (Maryland), `NC` (North Carolina), `NE` (Nebraska), `NJ` (New Jersey), `NY` (New York), `OH` (Ohio), `PA` (Pennsylvania), `SC` (South Carolina), `TX` (Texas), and `WA` (Washington -- **my home state!**).  

***

The following code generates a plot that shows the time trends of the percent of people within each state responding with `Excellent` when asked how they perceive their overall health.  

```{r brfss_plot_1, warning = FALSE, fig.width = 8, fig.height = 6, dpi = 200, fig.align = 'center'}
brfss %>% 
    filter(response == "Excellent") %>% 
    group_by(year, state) %>% 
    summarize(
        mean_pct = mean(data_value_pct)) %>% 
    ggplot(
        aes(x = year, y = mean_pct, group = state, colour = state)) +
        geom_line() +
        labs(
            title = "Self-Perceived 'Overall Health' Status",
            subtitle = "US states average between 17.5 and 27.5% for years 2002-2010",
            x = "Year",
            y = "Mean % of People Indicating 'Excellent' Overall Health")
```

As we can see from the generated graphic, the mean percentage of people indicating `Excellent` overall health across all US states falls between ~12% at the lowest point and ~29% at the highest. However, there is a high density of mean percentages that fall between 17.5% and 27.5%. Come on, America, we can do better!

***

The following code generates comparative plots (by years `2006` and `2010`) for the five responses (`Poor` to `Excellent`) distribution among locations in NY State.

```{r brfss_plot_2, fig.width = 8, fig.height = 6, dpi = 200, fig.align = 'center'}
brfss %>% 
    mutate(county = str_to_title(county)) %>% 
    filter(
        year %in% c(2006, 2010),
        state == "NY") %>% 
        group_by(year, county) %>%
    ggplot(aes(x = response, y = data_value_pct, fill = county)) + 
    geom_col(position = "dodge") + 
    facet_grid(. ~ year) +
    viridis::scale_fill_viridis(
        option = "cividis",
        name = "county",
        discrete = TRUE) +
    labs(
        title = "Levels of 'Overall Health' Differences by NY County",
        subtitle = "*Bronx, Erie, and Monroe counties included in 2010 only",
        x = "Response Categories",
        y = "Percentage of Sampled Individuals' Response Selection") +
    theme(legend.position = "bottom")
```

It's important for us to note that Bronx, Erie, and Monroe counties were not included in the 2006 data. However, we see from 2006 to 2010, the rates of individuals indicating that they had `Very good` overall health rose significantly for all counties included in both years. This is especially evident for Westchester and Nassau counties.

***

## Problem #3
###### (*25 points*)
##### *Exploration of Accelerometer Data from a 63 Year-Old Male with BMI 25*

```{r import_data_accelerometer, message = FALSE}
accel_data <- read_csv("./data/accel_data.csv")
```

```{r tidy_and_wrangle_accel_data, eval = FALSE}
#THIS CODE CHUNK IS A WORK IN PROGRESS (AS OF 2019-10-13 10:02)
accel_data %>% 
    pivot_longer(
        activity.1:activity.1440,
        names_to = "activity",
        values_to = "activity_count") %>% 
    mutate(
        day_type = case_when(
            day %in% c("Saturday", "Sunday") ~ "weekend",
            TRUE ~ "weekday")) %>% 
    group_by(day_id) %>% 
    summarize(day_total = sum(activity_count)) %>% 
    pivot_wider(
        names_from = day_id,
        values_from = day_total)
```

***

## Bonus Content

> "Oh, so you think darkness is your ally? But you merely adopted the dark. I was born in it, molded by it." -[Bane Cat.](https://youtu.be/5ywjpbThDpE)  

<center> ![](bane_cat.jpg) </center>